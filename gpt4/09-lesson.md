## ðŸ§  9â€“Dars: LangChain bilan End-to-End RAG Tizimi Qurish

---

### ðŸŽ¯ Maqsad:

* `LangChain` frameworkâ€™ida **toâ€˜liq RAG tizimi** yaratish
* PDF yoki matnli faylni yuklab, uni retriever + generator bilan bogâ€˜lash
* Faiss bilan local vector bazadan qidiruv qilish
* GPT-4 yoki OpenAI modeli bilan javob generatsiya qilish

---

## ðŸ“¦ RAG tizimi oqimi (LangChain versiyasi)

```mermaid
graph TD
    A[Fayl (PDF / txt)] --> B[Chunking & Embedding]
    B --> C[FAISS index]
    D[User Query] --> E[Retriever (Top-N)]
    C --> E
    E --> F[Prompt yaratish]
    F --> G[OpenAI / LLM]
    G --> H[Javob chiqarish]
```

---

## âš™ï¸ 1. Muhitni sozlash

```bash
pip install langchain openai faiss-cpu tiktoken sentence-transformers
```

ðŸ‘‰ OpenAI API key kerak:

```bash
export OPENAI_API_KEY=sk-...
```

---

## ðŸ“‚ 2. Asosiy modullarni import qilish

```python
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.llms import OpenAI
from langchain.chains import RetrievalQA
```

---

## ðŸ“„ 3. Hujjatni yuklash va chunklash

```python
loader = TextLoader("my_docs.txt", encoding='utf-8')
docs = loader.load()

splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
chunks = splitter.split_documents(docs)
```

---

## ðŸ§  4. Embedding + FAISS index yaratish

```python
embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
db = FAISS.from_documents(chunks, embeddings)
db.save_local("rag_index")
```

---

## ðŸ” 5. Retriever va LLMâ€™ni bogâ€˜lash

```python
retriever = db.as_retriever(search_type="similarity", search_kwargs={"k": 4})
llm = OpenAI(model_name="gpt-3.5-turbo", temperature=0)

rag_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",  # "stuff" = all docs in one prompt
    retriever=retriever,
    return_source_documents=True
)
```

---

## â“ 6. Soâ€˜rov yuborish

```python
query = "O'zbekistonning internet tezligi haqida nima deyilgan?"
result = rag_chain(query)

print("Javob:")
print(result['result'])

print("\nManbalar:")
for doc in result['source_documents']:
    print("-", doc.metadata.get("source", "Nomaâ€™lum"))
```

---

## âœ… Endi Senda Ishlaydigan RAG Tizim Bor

### Foydalanuvchi:

> "GPT haqida nima deyilgan?"

### RAG tizimi:

> "GPT bu OpenAI tomonidan ishlab chiqilgan generativ model boâ€˜lib, kontekst asosida javob bera oladi."

### Source:

* my\_docs.txt â†’ `chunk_2`

---

## ðŸ”¥ Keyingi level (oâ€˜rganish uchun):

* Vector DB sifatida Qdrant ishlatish (LangChain qoâ€˜llab-quvvatlaydi)
* Prompt templateâ€™ni sozlash
* Feedback loop yoki reranking qoâ€˜shish
* PDF, DOCX, YouTube transcript kabi murakkab loaderâ€™lar

---

## âœ… Uyga vazifa:

1. `.txt` yoki `.pdf` fayldan RAG pipeline qur
2. RAG bilan savol soâ€˜rab, manbani chiqaruvchi bot yarat
3. `chain_type="map_reduce"` yoki `refine` modellarini test qilib koâ€˜r
